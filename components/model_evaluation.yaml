name: Model evaluation
description: Evaluates the model and saves metrics to a JSON file.
inputs:
- {name: test_csv, type: String}
- {name: model_pkl, type: String}
outputs:
- {name: metrics_json, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'pandas' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
      --quiet --no-warn-script-location 'pandas' 'scikit-learn' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def model_evaluation(test_csv ,
                           model_pkl ,
                           metrics_json ):
          """
          Evaluates the model and saves metrics to a JSON file.
          """
          import pandas as pd
          import pickle
          import json
          from sklearn.metrics import mean_squared_error, r2_score

          print("Evaluation: Loading test data and model...")
          df = pd.read_csv(test_csv)
          X_test = df.drop('MEDV', axis=1)
          y_test = df['MEDV']

          with open(model_pkl, 'rb') as f:
              model = pickle.load(f)

          # Predict and Calculate Metrics
          predictions = model.predict(X_test)
          mse = mean_squared_error(y_test, predictions)
          r2 = r2_score(y_test, predictions)

          print(f"Evaluation Results -> MSE: {mse}, R2: {r2}")

          # Save metrics in Kubeflow format
          metrics = {
              "metrics": [
                  {"name": "Mean Squared Error", "numberValue": mse},
                  {"name": "R2 Score", "numberValue": r2},
              ]
          }

          with open(metrics_json, 'w') as f:
              json.dump(metrics, f)

      import argparse
      _parser = argparse.ArgumentParser(prog='Model evaluation', description='Evaluates the model and saves metrics to a JSON file.')
      _parser.add_argument("--test-csv", dest="test_csv", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model-pkl", dest="model_pkl", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--metrics-json", dest="metrics_json", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = model_evaluation(**_parsed_args)
    args:
    - --test-csv
    - {inputPath: test_csv}
    - --model-pkl
    - {inputPath: model_pkl}
    - --metrics-json
    - {outputPath: metrics_json}
